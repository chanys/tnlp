task: seq2seq_ner

model:
    model_id: google/flan-t5-base
    tokenizer_id: google/flan-t5-base

hyperparams:
    batch_size: 32
    gradient_accumulation_steps: 1
    epoch: 3
    learning_rate: 0.001
    warmup_steps: 0
    save_steps: 10000
    save_total_limit: 0
    max_context_length: 256
    max_response_length: 128

lora:
    r: 8
    alpha: 16
    target_modules:
        - q
        - v
    task_type: SEQ_2_SEQ_LM

processing:
    output_dir: BASE_DIR/expts/conll2003_seq2seq
    seed: 42

data:
    train_jsonl: BASE_DIR/data/conll2003/train.jsonl
    validation_jsonl: BASE_DIR/data/conll2003/validation.jsonl
    test_jsonl: BASE_DIR/data/conll2003/test.jsonl
    inference_jsonl: BASE_DIR/data/conll2003/inference.jsonl

prompt:
    context_prompt: "{}"
    response_prompt: "{}"


