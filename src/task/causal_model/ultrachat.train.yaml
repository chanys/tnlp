task: chat_ft

model:
    model_id: mistralai/Mistral-7B-v0.1
    tokenizer_id: mistralai/Mistral-7B-v0.1

hyperparams:
    per_device_eval_batch_size: 8
    per_device_train_batch_size: 4
    gradient_accumulation_steps: 4
    epoch: 1
    learning_rate: 0.00002
    warmup_steps: 0
    save_steps: 10000
    save_total_limit: 0
    max_seq_length: 1024

lora:
    r: 16
    alpha: 16
    target_modules:
        - q_proj
        - k_proj
        - v_proj
        - o_proj
    task_type: CAUSAL_LM

processing:
    output_dir: /home/chanys/tnlp/expts/ultrachat_sft
    merged_model_output_dir: /home/chanys/tnlp/expts/ultrachat_merged_model
    seed: 42

data:
    train_jsonl: /home/chanys/tnlp/data/ultrachat/train_sft.jsonl
    validation_jsonl: /home/chanys/tnlp/data/ultrachat/test_sft.jsonl
    test_jsonl: /home/chanys/tnlp/data/ultrachat/test_sft.jsonl

